{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mike del\n",
      "[nltk_data]     Castillo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Mike del\n",
      "[nltk_data]     Castillo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    " \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def remove_stop_words(text):\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for', 'at', 'too', 'most', 'if', 'this', \"you're\", 'our', 'wouldn', 'only', 'don', 'been', 'y', \"needn't\", 'once', 'same', 'theirs', 'ain', 'or', 'your', 'all', 'very', 'me', 'needn', 'than', 'how', 'were', 'doing', 'over', 'she', 're', 'couldn', 'doesn', 'didn', 'between', 't', 'is', \"it's\", 'him', 'which', 'ourselves', 'other', 'why', 'his', 'has', 'aren', 'the', 'its', 'by', 'few', \"isn't\", \"shan't\", \"haven't\", 'there', 'out', \"wouldn't\", 'have', 'you', 'hasn', 'each', 'shan', 'about', 'should', 'below', 'o', 'after', 'an', 'any', 'ours', 'when', 'themselves', \"mightn't\", 'itself', 'down', \"hadn't\", 'shouldn', 'my', 'wasn', 'being', 'herself', 'm', 'no', 'can', 'mightn', \"mustn't\", 'weren', 'they', 'just', 'where', 'again', 'hadn', 'what', 'yourself', \"you've\", 'against', 'won', 'until', 'a', 'having', 'further', 'them', \"doesn't\", 'himself', 'then', 'hers', 'd', 'and', 'isn', 'it', 'had', 'now', 'more', 'whom', 'll', \"couldn't\", 'myself', 's', 'we', 'because', 'nor', \"should've\", 'mustn', \"didn't\", 'from', 'as', \"wasn't\", 'while', 'some', 'did', 'was', 'in', \"that'll\", 'such', 'up', 'their', 'here', 'yourselves', \"you'll\", \"weren't\", 'ma', 'yours', 'these', 'so', 'with', \"she's\", 'he', 'i', 'are', \"don't\", 'those', 'haven', 'am', 'her', 'does', 've', 'do', \"shouldn't\", 'into', 'but', 'during', 'who', 'own', 'to', 'on', \"hasn't\", \"you'd\", 'through', 'of', \"aren't\", 'above', 'before', 'off', 'under', 'be', 'will', 'both', 'that', 'not', \"won't\"}\n",
      "['wouldn', \"needn't\", 'couldn', 'didn', 'aren', \"isn't\", \"shan't\", \"haven't\", \"wouldn't\", 'hasn', 'shan', \"mightn't\", \"hadn't\", 'wasn', 'shouldn', \"shouldn't\", 'not', \"won'thasn't\", \"aren't\", 'haven', 'mustn', \"doesn't\"]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)\n",
    "\n",
    "whitelist = [\n",
    "    'wouldn',\n",
    "    \"needn't\",\n",
    "    'couldn',\n",
    "    'didn',\n",
    "    'aren',\n",
    "    \"isn't\", \"shan't\", \"haven't\",\n",
    "    \"wouldn't\",\n",
    "    'hasn',\n",
    "    'shan',\n",
    "    \"mightn't\",\n",
    "    \"hadn't\",\n",
    "    'wasn',\n",
    "    'shouldn',\n",
    "    \"shouldn't\",\n",
    "    'not', \"won't\"\n",
    "    \"hasn't\",\n",
    "    \"aren't\",\n",
    "    'haven',\n",
    "    'mustn',\n",
    "    \"doesn't\",\n",
    "]\n",
    "\n",
    "print(whitelist)\n",
    "\n",
    "print([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'testing see works hooray'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stop_words(\"testing to see if this works hooray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/twitter-airline-sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plus youve added commercials experience tacky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>didnt today must mean need take another trip</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>really big bad thing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  pos  neg\n",
       "0                                               said  0.0  0.0\n",
       "1      plus youve added commercials experience tacky  1.0  0.0\n",
       "2       didnt today must mean need take another trip  0.0  0.0\n",
       "3  really aggressive blast obnoxious entertainmen...  0.0  1.0\n",
       "4                               really big bad thing  0.0  1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = list(map(remove_stop_words, data['text'].values))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = GaussianNB()\n",
    "classifier = RandomForestClassifier()\n",
    "# vectorizer = TfidfVectorizer()\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "            np.array(data.text), \n",
    "            np.array(data.pos),\n",
    "            test_size=0.25, \n",
    "            random_state=2574)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_x = vectorizer.fit_transform(train_x)\n",
    "classifier.fit(tfidf_train_x.toarray(), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous accuracy: 87.17 percent\n"
     ]
    }
   ],
   "source": [
    "tfidf_test_x = vectorizer.transform(test_x)\n",
    "scores = cross_val_score(classifier, tfidf_test_x.toarray(), test_y, cv=5)\n",
    "acc = scores.mean()\n",
    "print(\"Previous accuracy: %0.2f percent\" % (acc *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woah that  ==  0.0\n",
      "fuck that  ==  0.0\n",
      "i think it was fine but that shit was horrible!  ==  0.0\n",
      "meg how are you?  ==  0.0\n",
      "i really did not like this!  ==  0.0\n",
      "i hated this  ==  0.0\n",
      "that's not really nice  ==  1.0\n",
      "i'm not into you  ==  0.0\n",
      "i want to try things out  ==  0.0\n",
      "i really think this will work out great!  ==  0.0\n",
      "this was great!  ==  1.0\n",
      "this was not great!  ==  1.0\n",
      "i didn't get how it worked? what is wrong?  ==  1.0\n",
      "your service is subpar  ==  0.0\n",
      "service  ==  0.0\n",
      "good  ==  0.0\n",
      "bad  ==  0.0\n",
      "hate  ==  0.0\n",
      "love  ==  1.0\n"
     ]
    }
   ],
   "source": [
    "mess = [\n",
    "    'woah that', \n",
    "    'fuck that',\n",
    "    'i think it was fine but that shit was horrible!',\n",
    "    'meg how are you?',\n",
    "    'i really did not like this!',\n",
    "    'i hated this',\n",
    "    \"that's not really nice\",\n",
    "    \"i'm not into you\",\n",
    "    \"i want to try things out\",\n",
    "    \"i really think this will work out great!\",\n",
    "    \"this was great!\",\n",
    "    \"this was not great!\",\n",
    "    \"i didn't get how it worked? what is wrong?\",\n",
    "    \"your service is subpar\",\n",
    "    \"service\",\n",
    "    \"good\",\n",
    "    \"bad\",    \n",
    "    \"hate\",\n",
    "    \"love\",\n",
    "\n",
    "]\n",
    "t = vectorizer.transform(list(map(remove_stop_words, mess))).toarray();\n",
    "output = classifier.predict(t)\n",
    "\n",
    "for i ,m in enumerate(mess):\n",
    "    print(m, ' == ', output[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
