{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk.corpus import stopwords\n",
    "np.random.seed(1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['only' 'y' 'by' 'am' 'most' 'me' 'same' 'these' 'so' 'some' 'why' 'down'\n",
      " 'had' 'd' 'at' 'having' 'those' 'has' 'few' 'theirs' \"you've\" 'more' 'i'\n",
      " 'than' 'through' 'be' 'what' 'where' 'myself' 'which' 'doing' 'ours'\n",
      " 'will' 'in' 'both' 'do' 'it' 'o' 'on' 'yours' 'once' 'ourselves' 'here'\n",
      " 'about' \"it's\" 'my' 'for' 'her' 'then' 'after' \"should've\" 'from' 'each'\n",
      " 'when' 'does' 'now' 'off' 'don' 'are' 'we' 'itself' 'should' 'his'\n",
      " 'between' 'our' 'were' 'under' 'other' 'all' 'she' 'won' 'been' \"you're\"\n",
      " 'how' 'did' 'yourself' 'they' 'into' 'there' 've' 'such' 't' 's' 'and'\n",
      " 'over' 'to' 'just' 'was' 'being' 'because' 'if' 'who' 'further' 'the'\n",
      " 'any' \"that'll\" 'themselves' 'as' 'again' \"you'd\" 'until' 'he' 'him'\n",
      " 'this' 'or' 'of' 'below' 'an' \"she's\" 'weren' 'm' 'their' 'ma' 'up' 'll'\n",
      " 'whom' 'hers' 'can' 'you' 'them' 'very' 'a' 'herself' 'before' 'too'\n",
      " 'himself' 'during' 're' 'out' 'its' 'above' 'own' 'have' 'while'\n",
      " 'yourselves' 'that' 'with' \"you'll\" 'is' 'your']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "stop_words = pd.read_csv('../data/stopwords.csv')['words'].values\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    word_tokens = text_to_word_sequence(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/twitter-airline-sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plus youve added commercials experience tacky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>didnt today must mean need take another trip</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>really big bad thing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  pos  neg\n",
       "0                                               said  0.0  0.0\n",
       "1      plus youve added commercials experience tacky  1.0  0.0\n",
       "2       didnt today must mean need take another trip  0.0  0.0\n",
       "3  really aggressive blast obnoxious entertainmen...  0.0  1.0\n",
       "4                               really big bad thing  0.0  1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = list(map(remove_stop_words, data['text'].values))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['text'].values\n",
    "corpus = [text_to_word_sequence(y) for y in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw_train, X_raw_test, Y_train, Y_test = train_test_split(\n",
    "    sentences,\n",
    "    data[['pos', 'neg']].values,\n",
    "    test_size=0.2, \n",
    "    random_state=3945\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 35\n",
    "vector_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "word2vec = Word2Vec(sentences=corpus,\n",
    "                    size=vector_size, \n",
    "                    window=10, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=4)\n",
    "word2vec = word2vec.wv\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2index(corpus):\n",
    "    gc.collect()\n",
    "    input_matrix = np.zeros((len(corpus),max_sentence_length))\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        for t, token in enumerate(corpus[i]):\n",
    "            if t >= max_sentence_length:\n",
    "                break\n",
    "            if token not in word2vec.vocab:\n",
    "                continue\n",
    "            input_matrix[i, t] = word2vec.vocab.get(token).index\n",
    "    return input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sent2index(X_raw_train)\n",
    "X_test = sent2index(X_raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2590, 300)\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "vocab_len = len(word2vec.vocab) + 1\n",
    "\n",
    "emb_matrix = np.zeros((vocab_len, vector_size))\n",
    "\n",
    "for word in word2vec.vocab:\n",
    "    index = word2vec.vocab.get(word).index\n",
    "    emb_matrix[index, :] = word2vec[word]\n",
    "    \n",
    "print(emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "sentence_indices = Input(shape=(max_sentence_length,))\n",
    "    \n",
    "embedding_layer = Embedding(vocab_len, vector_size, trainable = False)\n",
    "embedding_layer.build((None,))\n",
    "embedding_layer.set_weights([emb_matrix])\n",
    "\n",
    "embeddings = embedding_layer(sentence_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35, 300)           777000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 35, 128)           219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,128,490\n",
      "Trainable params: 351,490\n",
      "Non-trainable params: 777,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X = LSTM(128, return_sequences=True)(embeddings)\n",
    "X = Dropout(0.5)(X)\n",
    "X = LSTM(128, return_sequences=False)(X)\n",
    "X = Dropout(0.5)(X)\n",
    "X = Dense(2, activation=None)(X)\n",
    "X = Activation('softmax')(X)\n",
    "\n",
    "model = Model(inputs=[sentence_indices], outputs=X)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11096 samples, validate on 2775 samples\n",
      "Epoch 1/20\n",
      "11096/11096 [==============================] - 175s 16ms/step - loss: 0.3959 - acc: 0.6393 - val_loss: 0.3891 - val_acc: 0.6256\n",
      "Epoch 2/20\n",
      "11096/11096 [==============================] - 171s 15ms/step - loss: 0.3855 - acc: 0.6393 - val_loss: 0.3810 - val_acc: 0.6256\n",
      "Epoch 3/20\n",
      "11096/11096 [==============================] - 195s 18ms/step - loss: 0.3710 - acc: 0.6421 - val_loss: 0.3572 - val_acc: 0.6418\n",
      "Epoch 4/20\n",
      "11096/11096 [==============================] - 188s 17ms/step - loss: 0.3565 - acc: 0.6453 - val_loss: 0.3559 - val_acc: 0.6512\n",
      "Epoch 5/20\n",
      "11096/11096 [==============================] - 178s 16ms/step - loss: 0.3459 - acc: 0.6550 - val_loss: 0.3526 - val_acc: 0.6378\n",
      "Epoch 6/20\n",
      "11096/11096 [==============================] - 227s 20ms/step - loss: 0.3291 - acc: 0.6643 - val_loss: 0.3425 - val_acc: 0.6468\n",
      "Epoch 7/20\n",
      "11096/11096 [==============================] - 276s 25ms/step - loss: 0.3250 - acc: 0.6708 - val_loss: 0.3521 - val_acc: 0.6580\n",
      "Epoch 8/20\n",
      "11096/11096 [==============================] - 197s 18ms/step - loss: 0.3180 - acc: 0.6712 - val_loss: 0.3574 - val_acc: 0.6465\n",
      "Epoch 9/20\n",
      "11096/11096 [==============================] - 189s 17ms/step - loss: 0.3097 - acc: 0.6768 - val_loss: 0.3442 - val_acc: 0.6472\n",
      "Epoch 10/20\n",
      "11096/11096 [==============================] - 172s 16ms/step - loss: 0.3067 - acc: 0.6791 - val_loss: 0.3383 - val_acc: 0.6515\n",
      "Epoch 11/20\n",
      "11096/11096 [==============================] - 189s 17ms/step - loss: 0.2989 - acc: 0.6834 - val_loss: 0.3465 - val_acc: 0.6497\n",
      "Epoch 12/20\n",
      "11096/11096 [==============================] - 185s 17ms/step - loss: 0.2968 - acc: 0.6865 - val_loss: 0.3661 - val_acc: 0.6461\n",
      "Epoch 13/20\n",
      "11096/11096 [==============================] - 278s 25ms/step - loss: 0.2909 - acc: 0.6843 - val_loss: 0.3397 - val_acc: 0.6688\n",
      "Epoch 14/20\n",
      "11096/11096 [==============================] - 192s 17ms/step - loss: 0.2806 - acc: 0.6939 - val_loss: 0.3400 - val_acc: 0.6688\n",
      "Epoch 15/20\n",
      "11096/11096 [==============================] - 198s 18ms/step - loss: 0.2726 - acc: 0.6989 - val_loss: 0.3474 - val_acc: 0.6649\n",
      "Epoch 16/20\n",
      "11096/11096 [==============================] - 179s 16ms/step - loss: 0.2687 - acc: 0.7041 - val_loss: 0.3469 - val_acc: 0.6670\n",
      "Epoch 17/20\n",
      "11096/11096 [==============================] - 195s 18ms/step - loss: 0.2581 - acc: 0.7130 - val_loss: 0.3424 - val_acc: 0.6760\n",
      "Epoch 18/20\n",
      "11096/11096 [==============================] - 204s 18ms/step - loss: 0.2557 - acc: 0.7120 - val_loss: 0.3432 - val_acc: 0.6609\n",
      "Epoch 19/20\n",
      "11096/11096 [==============================] - 226s 20ms/step - loss: 0.2456 - acc: 0.7158 - val_loss: 0.3543 - val_acc: 0.6721\n",
      "Epoch 20/20\n",
      " 8912/11096 [=======================>......] - ETA: 45s - loss: 0.2317 - acc: 0.7236"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y=Y_train, \n",
    "    batch_size=16, \n",
    "    epochs=20, \n",
    "    verbose=1, \n",
    "    validation_data=(X_test, Y_test), \n",
    "#     callbacks=[earlystop],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess = np.array([\n",
    "    'this is slow',\n",
    "    'this is exceptional service',\n",
    "])\n",
    "X_test_indices = sent2index(mess)\n",
    "pred = model.predict(X_test_indices)\n",
    "\n",
    "output = ''\n",
    "for i ,m in enumerate(mess):\n",
    "        output += ('{} {} {}\\n'.format('POSITIVE:' if pred[i][0] > 0.5 else 'NEGATIVE:', m, pred[i]))\n",
    "        \n",
    "print(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
