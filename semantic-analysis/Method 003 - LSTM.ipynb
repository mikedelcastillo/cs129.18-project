{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk.corpus import stopwords\n",
    "np.random.seed(1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['only' 'y' 'by' 'am' 'most' 'me' 'same' 'these' 'so' 'some' 'why' 'down'\n",
      " 'had' 'd' 'at' 'having' 'those' 'has' 'few' 'theirs' \"you've\" 'more' 'i'\n",
      " 'than' 'through' 'be' 'what' 'where' 'myself' 'which' 'doing' 'ours'\n",
      " 'will' 'in' 'both' 'do' 'it' 'o' 'on' 'yours' 'once' 'ourselves' 'here'\n",
      " 'about' \"it's\" 'my' 'for' 'her' 'then' 'after' \"should've\" 'from' 'each'\n",
      " 'when' 'does' 'now' 'off' 'don' 'are' 'we' 'itself' 'should' 'his'\n",
      " 'between' 'our' 'were' 'under' 'other' 'all' 'she' 'won' 'been' \"you're\"\n",
      " 'how' 'did' 'yourself' 'they' 'into' 'there' 've' 'such' 't' 's' 'and'\n",
      " 'over' 'to' 'just' 'was' 'being' 'because' 'if' 'who' 'further' 'the'\n",
      " 'any' \"that'll\" 'themselves' 'as' 'again' \"you'd\" 'until' 'he' 'him'\n",
      " 'this' 'or' 'of' 'below' 'an' \"she's\" 'weren' 'm' 'their' 'ma' 'up' 'll'\n",
      " 'whom' 'hers' 'can' 'you' 'them' 'very' 'a' 'herself' 'before' 'too'\n",
      " 'himself' 'during' 're' 'out' 'its' 'above' 'own' 'have' 'while'\n",
      " 'yourselves' 'that' 'with' \"you'll\" 'is' 'your']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "stop_words = pd.read_csv('../data/stopwords.csv')['words'].values\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    word_tokens = text_to_word_sequence(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/twitter-airline-sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plus youve added commercials experience tacky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>didnt today must mean need take another trip</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>really big bad thing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  pos  neg\n",
       "0                                               said  0.0  0.0\n",
       "1      plus youve added commercials experience tacky  1.0  0.0\n",
       "2       didnt today must mean need take another trip  0.0  0.0\n",
       "3  really aggressive blast obnoxious entertainmen...  0.0  1.0\n",
       "4                               really big bad thing  0.0  1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = list(map(remove_stop_words, data['text'].values))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['text'].values\n",
    "corpus = [text_to_word_sequence(y) for y in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw_train, X_raw_test, Y_train, Y_test = train_test_split(\n",
    "    sentences,\n",
    "    data[['pos', 'neg']].values,\n",
    "    test_size=0.2, \n",
    "    random_state=3945\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 35\n",
    "vector_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "word2vec = Word2Vec(sentences=corpus,\n",
    "                    size=vector_size, \n",
    "                    window=10, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=4)\n",
    "word2vec = word2vec.wv\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2index(corpus):\n",
    "    gc.collect()\n",
    "    input_matrix = np.zeros((len(corpus),max_sentence_length))\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        for t, token in enumerate(corpus[i]):\n",
    "            if t >= max_sentence_length:\n",
    "                break\n",
    "            if token not in word2vec.vocab:\n",
    "                continue\n",
    "            input_matrix[i, t] = word2vec.vocab.get(token).index\n",
    "    return input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sent2index(X_raw_train)\n",
    "X_test = sent2index(X_raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2590, 300)\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "vocab_len = len(word2vec.vocab) + 1\n",
    "\n",
    "emb_matrix = np.zeros((vocab_len, vector_size))\n",
    "\n",
    "for word in word2vec.vocab:\n",
    "    index = word2vec.vocab.get(word).index\n",
    "    emb_matrix[index, :] = word2vec[word]\n",
    "    \n",
    "print(emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "sentence_indices = Input(shape=(max_sentence_length,))\n",
    "    \n",
    "embedding_layer = Embedding(vocab_len, vector_size, trainable = False)\n",
    "embedding_layer.build((None,))\n",
    "embedding_layer.set_weights([emb_matrix])\n",
    "\n",
    "embeddings = embedding_layer(sentence_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35, 300)           777000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 35, 128)           219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,128,490\n",
      "Trainable params: 351,490\n",
      "Non-trainable params: 777,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X = LSTM(128, return_sequences=True)(embeddings)\n",
    "X = Dropout(0.5)(X)\n",
    "X = LSTM(128, return_sequences=False)(X)\n",
    "X = Dropout(0.5)(X)\n",
    "X = Dense(2, activation=None)(X)\n",
    "X = Activation('softmax')(X)\n",
    "\n",
    "model = Model(inputs=[sentence_indices], outputs=X)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11096 samples, validate on 2775 samples\n",
      "Epoch 1/100\n",
      "11096/11096 [==============================] - 44s 4ms/step - loss: 0.3926 - acc: 0.6389 - val_loss: 0.3873 - val_acc: 0.6256\n",
      "Epoch 2/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.3843 - acc: 0.6391 - val_loss: 0.3797 - val_acc: 0.6256\n",
      "Epoch 3/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.3789 - acc: 0.6414 - val_loss: 0.3800 - val_acc: 0.6256\n",
      "Epoch 4/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.3677 - acc: 0.6442 - val_loss: 0.3671 - val_acc: 0.6429\n",
      "Epoch 5/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.3569 - acc: 0.6493 - val_loss: 0.3484 - val_acc: 0.6465\n",
      "Epoch 6/100\n",
      "11096/11096 [==============================] - 43s 4ms/step - loss: 0.3418 - acc: 0.6574 - val_loss: 0.3607 - val_acc: 0.6306\n",
      "Epoch 7/100\n",
      "11096/11096 [==============================] - 43s 4ms/step - loss: 0.3319 - acc: 0.6647 - val_loss: 0.3415 - val_acc: 0.6555\n",
      "Epoch 8/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.3240 - acc: 0.6688 - val_loss: 0.3614 - val_acc: 0.6569\n",
      "Epoch 9/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.3187 - acc: 0.6742 - val_loss: 0.3503 - val_acc: 0.6497\n",
      "Epoch 10/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.3090 - acc: 0.6794 - val_loss: 0.3468 - val_acc: 0.6595\n",
      "Epoch 11/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.3007 - acc: 0.6875 - val_loss: 0.3461 - val_acc: 0.6602\n",
      "Epoch 12/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2939 - acc: 0.6906 - val_loss: 0.3603 - val_acc: 0.6587\n",
      "Epoch 13/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2859 - acc: 0.6935 - val_loss: 0.3318 - val_acc: 0.6685\n",
      "Epoch 14/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2812 - acc: 0.6996 - val_loss: 0.3434 - val_acc: 0.6688\n",
      "Epoch 15/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2758 - acc: 0.7009 - val_loss: 0.3501 - val_acc: 0.6598\n",
      "Epoch 16/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2690 - acc: 0.7064 - val_loss: 0.3514 - val_acc: 0.6710\n",
      "Epoch 17/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2618 - acc: 0.7089 - val_loss: 0.3434 - val_acc: 0.6699\n",
      "Epoch 18/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2547 - acc: 0.7131 - val_loss: 0.3565 - val_acc: 0.6757\n",
      "Epoch 19/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2486 - acc: 0.7174 - val_loss: 0.3570 - val_acc: 0.6717\n",
      "Epoch 20/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2416 - acc: 0.7203 - val_loss: 0.3720 - val_acc: 0.6724\n",
      "Epoch 21/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2366 - acc: 0.7210 - val_loss: 0.3806 - val_acc: 0.6677\n",
      "Epoch 22/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2322 - acc: 0.7232 - val_loss: 0.3857 - val_acc: 0.6706\n",
      "Epoch 23/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2254 - acc: 0.7285 - val_loss: 0.3731 - val_acc: 0.6717\n",
      "Epoch 24/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2172 - acc: 0.7339 - val_loss: 0.3705 - val_acc: 0.6724\n",
      "Epoch 25/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2177 - acc: 0.7306 - val_loss: 0.3936 - val_acc: 0.6706\n",
      "Epoch 26/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2125 - acc: 0.7356 - val_loss: 0.4075 - val_acc: 0.6753\n",
      "Epoch 27/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.2026 - acc: 0.7397 - val_loss: 0.4027 - val_acc: 0.6782\n",
      "Epoch 28/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1949 - acc: 0.7451 - val_loss: 0.4050 - val_acc: 0.6692\n",
      "Epoch 29/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1921 - acc: 0.7470 - val_loss: 0.4560 - val_acc: 0.6782\n",
      "Epoch 30/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1840 - acc: 0.7481 - val_loss: 0.4439 - val_acc: 0.6681\n",
      "Epoch 31/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1813 - acc: 0.7500 - val_loss: 0.4830 - val_acc: 0.6750\n",
      "Epoch 32/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1791 - acc: 0.7503 - val_loss: 0.4318 - val_acc: 0.6706\n",
      "Epoch 33/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1737 - acc: 0.7536 - val_loss: 0.4810 - val_acc: 0.6695\n",
      "Epoch 34/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1736 - acc: 0.7539 - val_loss: 0.4810 - val_acc: 0.6786\n",
      "Epoch 35/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1701 - acc: 0.7559 - val_loss: 0.4441 - val_acc: 0.6764\n",
      "Epoch 36/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1612 - acc: 0.7631 - val_loss: 0.5275 - val_acc: 0.6721\n",
      "Epoch 37/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1509 - acc: 0.7643 - val_loss: 0.5103 - val_acc: 0.6739\n",
      "Epoch 38/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1512 - acc: 0.7651 - val_loss: 0.5499 - val_acc: 0.6670\n",
      "Epoch 39/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1498 - acc: 0.7665 - val_loss: 0.5246 - val_acc: 0.6739\n",
      "Epoch 40/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1443 - acc: 0.7651 - val_loss: 0.5972 - val_acc: 0.6742\n",
      "Epoch 41/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1400 - acc: 0.7682 - val_loss: 0.5370 - val_acc: 0.6786\n",
      "Epoch 42/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1395 - acc: 0.7695 - val_loss: 0.5424 - val_acc: 0.6818\n",
      "Epoch 43/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1357 - acc: 0.7685 - val_loss: 0.5925 - val_acc: 0.6728\n",
      "Epoch 44/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1271 - acc: 0.7754 - val_loss: 0.5864 - val_acc: 0.6695\n",
      "Epoch 45/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1285 - acc: 0.7754 - val_loss: 0.5741 - val_acc: 0.6652\n",
      "Epoch 46/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1285 - acc: 0.7742 - val_loss: 0.5479 - val_acc: 0.6721\n",
      "Epoch 47/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1231 - acc: 0.7797 - val_loss: 0.5756 - val_acc: 0.6692\n",
      "Epoch 48/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1214 - acc: 0.7806 - val_loss: 0.5998 - val_acc: 0.6703\n",
      "Epoch 49/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1190 - acc: 0.7801 - val_loss: 0.6052 - val_acc: 0.6667\n",
      "Epoch 50/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1143 - acc: 0.7798 - val_loss: 0.6381 - val_acc: 0.6638\n",
      "Epoch 51/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1129 - acc: 0.7817 - val_loss: 0.6421 - val_acc: 0.6602\n",
      "Epoch 52/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1140 - acc: 0.7797 - val_loss: 0.6364 - val_acc: 0.6739\n",
      "Epoch 53/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1103 - acc: 0.7863 - val_loss: 0.7448 - val_acc: 0.6652\n",
      "Epoch 54/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1125 - acc: 0.7806 - val_loss: 0.6599 - val_acc: 0.6732\n",
      "Epoch 55/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1129 - acc: 0.7825 - val_loss: 0.6381 - val_acc: 0.6562\n",
      "Epoch 56/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1067 - acc: 0.7830 - val_loss: 0.6750 - val_acc: 0.6638\n",
      "Epoch 57/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.1016 - acc: 0.7887 - val_loss: 0.6974 - val_acc: 0.6677\n",
      "Epoch 58/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0881 - acc: 0.7929 - val_loss: 0.7555 - val_acc: 0.6613\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0941 - acc: 0.7909 - val_loss: 0.7685 - val_acc: 0.6663\n",
      "Epoch 60/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0984 - acc: 0.7876 - val_loss: 0.6361 - val_acc: 0.6627\n",
      "Epoch 61/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0971 - acc: 0.7886 - val_loss: 0.6448 - val_acc: 0.6591\n",
      "Epoch 62/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0964 - acc: 0.7931 - val_loss: 0.6829 - val_acc: 0.6732\n",
      "Epoch 63/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0949 - acc: 0.7903 - val_loss: 0.6982 - val_acc: 0.6573\n",
      "Epoch 64/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0957 - acc: 0.7880 - val_loss: 0.7221 - val_acc: 0.6649\n",
      "Epoch 65/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0876 - acc: 0.7970 - val_loss: 0.7251 - val_acc: 0.6638\n",
      "Epoch 66/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0858 - acc: 0.7933 - val_loss: 0.7704 - val_acc: 0.6724\n",
      "Epoch 67/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0875 - acc: 0.7922 - val_loss: 0.7542 - val_acc: 0.6627\n",
      "Epoch 68/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0895 - acc: 0.7949 - val_loss: 0.7978 - val_acc: 0.6602\n",
      "Epoch 69/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0925 - acc: 0.7922 - val_loss: 0.7235 - val_acc: 0.6692\n",
      "Epoch 70/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0891 - acc: 0.7928 - val_loss: 0.6584 - val_acc: 0.6674\n",
      "Epoch 71/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0910 - acc: 0.7941 - val_loss: 0.7181 - val_acc: 0.6638\n",
      "Epoch 72/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0834 - acc: 0.7964 - val_loss: 0.7546 - val_acc: 0.6659\n",
      "Epoch 73/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0821 - acc: 0.7990 - val_loss: 0.7318 - val_acc: 0.6667\n",
      "Epoch 74/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0815 - acc: 0.7955 - val_loss: 0.7617 - val_acc: 0.6548\n",
      "Epoch 75/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0727 - acc: 0.8022 - val_loss: 0.7805 - val_acc: 0.6652\n",
      "Epoch 76/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0758 - acc: 0.7995 - val_loss: 0.7776 - val_acc: 0.6605\n",
      "Epoch 77/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0890 - acc: 0.7947 - val_loss: 0.7868 - val_acc: 0.6595\n",
      "Epoch 78/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0793 - acc: 0.7984 - val_loss: 0.8222 - val_acc: 0.6677\n",
      "Epoch 79/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0734 - acc: 0.8031 - val_loss: 0.8007 - val_acc: 0.6623\n",
      "Epoch 80/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0812 - acc: 0.7942 - val_loss: 0.7624 - val_acc: 0.6771\n",
      "Epoch 81/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0770 - acc: 0.7974 - val_loss: 0.7758 - val_acc: 0.6595\n",
      "Epoch 82/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0699 - acc: 0.8016 - val_loss: 0.8274 - val_acc: 0.6667\n",
      "Epoch 83/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0786 - acc: 0.8019 - val_loss: 0.7782 - val_acc: 0.6613\n",
      "Epoch 84/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0752 - acc: 0.7997 - val_loss: 0.8242 - val_acc: 0.6652\n",
      "Epoch 85/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0771 - acc: 0.7981 - val_loss: 0.7977 - val_acc: 0.6695\n",
      "Epoch 86/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0710 - acc: 0.7980 - val_loss: 0.7760 - val_acc: 0.6688\n",
      "Epoch 87/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0724 - acc: 0.7998 - val_loss: 0.7945 - val_acc: 0.6649\n",
      "Epoch 88/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0688 - acc: 0.8061 - val_loss: 0.8591 - val_acc: 0.6602\n",
      "Epoch 89/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0739 - acc: 0.8026 - val_loss: 0.8210 - val_acc: 0.6645\n",
      "Epoch 90/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0689 - acc: 0.8015 - val_loss: 0.7669 - val_acc: 0.6710\n",
      "Epoch 91/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0674 - acc: 0.8036 - val_loss: 0.8517 - val_acc: 0.6667\n",
      "Epoch 92/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0576 - acc: 0.8058 - val_loss: 0.9220 - val_acc: 0.6541\n",
      "Epoch 93/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0641 - acc: 0.8029 - val_loss: 0.8529 - val_acc: 0.6620\n",
      "Epoch 94/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0871 - acc: 0.7982 - val_loss: 0.7515 - val_acc: 0.6591\n",
      "Epoch 95/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0727 - acc: 0.8013 - val_loss: 0.8550 - val_acc: 0.6688\n",
      "Epoch 96/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0648 - acc: 0.8046 - val_loss: 0.8008 - val_acc: 0.6789\n",
      "Epoch 97/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0677 - acc: 0.8043 - val_loss: 0.8355 - val_acc: 0.6732\n",
      "Epoch 98/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0586 - acc: 0.8038 - val_loss: 0.9095 - val_acc: 0.6649\n",
      "Epoch 99/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0731 - acc: 0.8010 - val_loss: 0.8150 - val_acc: 0.6667\n",
      "Epoch 100/100\n",
      "11096/11096 [==============================] - 42s 4ms/step - loss: 0.0668 - acc: 0.8026 - val_loss: 0.8326 - val_acc: 0.6674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f7e93050f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y=Y_train, \n",
    "    batch_size=32, \n",
    "    epochs=100, \n",
    "    verbose=1, \n",
    "    validation_data=(X_test, Y_test), \n",
    "#     callbacks=[earlystop],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE: this is slow [0.1688826  0.83111745]\n",
      "POSITIVE: this is exceptional service [9.9912626e-01 8.7372388e-04]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mess = np.array([\n",
    "    'this is slow',\n",
    "    'this is exceptional service',\n",
    "])\n",
    "X_test_indices = sent2index(mess)\n",
    "pred = model.predict(X_test_indices)\n",
    "\n",
    "output = ''\n",
    "for i ,m in enumerate(mess):\n",
    "        output += ('{} {} {}\\n'.format('POSITIVE:' if pred[i][0] > 0.5 else 'NEGATIVE:', m, pred[i]))\n",
    "        \n",
    "print(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
