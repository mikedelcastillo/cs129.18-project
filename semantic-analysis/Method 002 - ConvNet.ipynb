{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import os.path\n",
    "import gc\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['only' 'y' 'by' 'am' 'most' 'me' 'same' 'these' 'so' 'some' 'why' 'down'\n",
      " 'had' 'd' 'at' 'having' 'those' 'has' 'few' 'theirs' \"you've\" 'more' 'i'\n",
      " 'than' 'through' 'be' 'what' 'where' 'myself' 'which' 'doing' 'ours'\n",
      " 'will' 'in' 'both' 'do' 'it' 'o' 'on' 'yours' 'once' 'ourselves' 'here'\n",
      " 'about' \"it's\" 'my' 'for' 'her' 'then' 'after' \"should've\" 'from' 'each'\n",
      " 'when' 'does' 'now' 'off' 'don' 'are' 'we' 'itself' 'should' 'his'\n",
      " 'between' 'our' 'were' 'under' 'other' 'all' 'she' 'won' 'been' \"you're\"\n",
      " 'how' 'did' 'yourself' 'they' 'into' 'there' 've' 'such' 't' 's' 'and'\n",
      " 'over' 'to' 'just' 'was' 'being' 'because' 'if' 'who' 'further' 'the'\n",
      " 'any' \"that'll\" 'themselves' 'as' 'again' \"you'd\" 'until' 'he' 'him'\n",
      " 'this' 'or' 'of' 'below' 'an' \"she's\" 'weren' 'm' 'their' 'ma' 'up' 'll'\n",
      " 'whom' 'hers' 'can' 'you' 'them' 'very' 'a' 'herself' 'before' 'too'\n",
      " 'himself' 'during' 're' 'out' 'its' 'above' 'own' 'have' 'while'\n",
      " 'yourselves' 'that' 'with' \"you'll\" 'is' 'your']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "stop_words = pd.read_csv('../data/stopwords.csv')['words'].values\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    word_tokens = text_to_word_sequence(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        allow_soft_placement=True, \n",
    "                        device_count = {'CPU' : 1, \n",
    "                                        'GPU' : 1 if use_gpu else 0})\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/twitter-airline-sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plus youve added commercials experience tacky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>didnt today must mean need take another trip</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>really big bad thing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  pos  neg\n",
       "0                                               said  0.0  0.0\n",
       "1      plus youve added commercials experience tacky  1.0  0.0\n",
       "2       didnt today must mean need take another trip  0.0  0.0\n",
       "3  really aggressive blast obnoxious entertainmen...  0.0  1.0\n",
       "4                               really big bad thing  0.0  1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = list(map(remove_stop_words, data['text'].values))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 13871\n"
     ]
    }
   ],
   "source": [
    "corpus = [text_to_word_sequence(y) for y in [x[0] for x in data[['text']].values]]\n",
    "labels = [np.array(x[[0, 1]]) for x in data[['pos', 'neg']].values]\n",
    "    \n",
    "print('Corpus size: {}'.format(len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 35\n",
    "vector_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec = Word2Vec(sentences=corpus,\n",
    "                    size=vector_size, \n",
    "                    window=10, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=multiprocessing.cpu_count())\n",
    "\n",
    "vecs_x = word2vec.wv\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_vec_data(corpus):\n",
    "    gc.collect()\n",
    "    input_matrix = np.zeros((len(corpus), max_sentence_length, vector_size), dtype=K.floatx())\n",
    "    for i in range(len(corpus)):\n",
    "        for t, token in enumerate(corpus[i]):\n",
    "            if t >= max_sentence_length:\n",
    "                break\n",
    "            if token not in vecs_x:\n",
    "                continue\n",
    "            input_matrix[i, t, :] = vecs_x[token]\n",
    "    return input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras convolutional model\n",
    "gc.collect()\n",
    "batch_size = 32\n",
    "nb_epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same', input_shape=(max_sentence_length, vector_size)))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.0001, decay=1e-6),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11096 samples, validate on 2775 samples\n",
      "Epoch 1/100\n",
      "11096/11096 [==============================] - 7s 623us/step - loss: 0.2772 - acc: 0.7074 - val_loss: 0.2210 - val_acc: 0.7398\n",
      "Epoch 2/100\n",
      "11096/11096 [==============================] - 4s 336us/step - loss: 0.2237 - acc: 0.7433 - val_loss: 0.2150 - val_acc: 0.7308\n",
      "Epoch 3/100\n",
      "11096/11096 [==============================] - 4s 332us/step - loss: 0.2084 - acc: 0.7492 - val_loss: 0.2134 - val_acc: 0.7319\n",
      "Epoch 4/100\n",
      "11096/11096 [==============================] - 4s 336us/step - loss: 0.2005 - acc: 0.7549 - val_loss: 0.2080 - val_acc: 0.7416\n",
      "Epoch 5/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.1910 - acc: 0.7566 - val_loss: 0.2167 - val_acc: 0.7337\n",
      "Epoch 6/100\n",
      "11096/11096 [==============================] - 4s 339us/step - loss: 0.1857 - acc: 0.7601 - val_loss: 0.2148 - val_acc: 0.7369\n",
      "Epoch 7/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.1783 - acc: 0.7628 - val_loss: 0.2196 - val_acc: 0.7344\n",
      "Epoch 8/100\n",
      "11096/11096 [==============================] - 4s 356us/step - loss: 0.1732 - acc: 0.7656 - val_loss: 0.2170 - val_acc: 0.7431\n",
      "Epoch 9/100\n",
      "11096/11096 [==============================] - 4s 342us/step - loss: 0.1625 - acc: 0.7686 - val_loss: 0.2204 - val_acc: 0.7387\n",
      "Epoch 10/100\n",
      "11096/11096 [==============================] - 4s 352us/step - loss: 0.1569 - acc: 0.7696 - val_loss: 0.2346 - val_acc: 0.7366\n",
      "Epoch 11/100\n",
      "11096/11096 [==============================] - 5s 411us/step - loss: 0.1474 - acc: 0.7790 - val_loss: 0.2358 - val_acc: 0.7384\n",
      "Epoch 12/100\n",
      "11096/11096 [==============================] - 4s 343us/step - loss: 0.1408 - acc: 0.7806 - val_loss: 0.2413 - val_acc: 0.7409\n",
      "Epoch 13/100\n",
      "11096/11096 [==============================] - 4s 344us/step - loss: 0.1317 - acc: 0.7883 - val_loss: 0.2472 - val_acc: 0.7402\n",
      "Epoch 14/100\n",
      "11096/11096 [==============================] - 4s 342us/step - loss: 0.1282 - acc: 0.7896 - val_loss: 0.2542 - val_acc: 0.7359\n",
      "Epoch 15/100\n",
      "11096/11096 [==============================] - 4s 341us/step - loss: 0.1237 - acc: 0.7881 - val_loss: 0.2656 - val_acc: 0.7362\n",
      "Epoch 16/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.1152 - acc: 0.7965 - val_loss: 0.2856 - val_acc: 0.7250\n",
      "Epoch 17/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.1102 - acc: 0.7952 - val_loss: 0.2816 - val_acc: 0.7384\n",
      "Epoch 18/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.1053 - acc: 0.7981 - val_loss: 0.2908 - val_acc: 0.7380\n",
      "Epoch 19/100\n",
      "11096/11096 [==============================] - 4s 340us/step - loss: 0.0997 - acc: 0.8008 - val_loss: 0.3148 - val_acc: 0.7315\n",
      "Epoch 20/100\n",
      "11096/11096 [==============================] - 4s 340us/step - loss: 0.0948 - acc: 0.8018 - val_loss: 0.3248 - val_acc: 0.7348\n",
      "Epoch 21/100\n",
      "11096/11096 [==============================] - 4s 339us/step - loss: 0.0923 - acc: 0.8051 - val_loss: 0.3238 - val_acc: 0.7333\n",
      "Epoch 22/100\n",
      "11096/11096 [==============================] - 4s 340us/step - loss: 0.0859 - acc: 0.8072 - val_loss: 0.3397 - val_acc: 0.7323\n",
      "Epoch 23/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.0823 - acc: 0.8091 - val_loss: 0.3558 - val_acc: 0.7211\n",
      "Epoch 24/100\n",
      "11096/11096 [==============================] - 4s 336us/step - loss: 0.0777 - acc: 0.8080 - val_loss: 0.3537 - val_acc: 0.7294\n",
      "Epoch 25/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.0772 - acc: 0.8106 - val_loss: 0.3514 - val_acc: 0.7348\n",
      "Epoch 26/100\n",
      "11096/11096 [==============================] - 4s 341us/step - loss: 0.0697 - acc: 0.8126 - val_loss: 0.3927 - val_acc: 0.7323\n",
      "Epoch 27/100\n",
      "11096/11096 [==============================] - 4s 336us/step - loss: 0.0688 - acc: 0.8145 - val_loss: 0.3778 - val_acc: 0.7373\n",
      "Epoch 28/100\n",
      "11096/11096 [==============================] - 4s 342us/step - loss: 0.0655 - acc: 0.8138 - val_loss: 0.4059 - val_acc: 0.7380\n",
      "Epoch 29/100\n",
      "11096/11096 [==============================] - 4s 348us/step - loss: 0.0626 - acc: 0.8161 - val_loss: 0.3963 - val_acc: 0.7283\n",
      "Epoch 30/100\n",
      "11096/11096 [==============================] - 4s 357us/step - loss: 0.0614 - acc: 0.8187 - val_loss: 0.4113 - val_acc: 0.7344\n",
      "Epoch 31/100\n",
      "11096/11096 [==============================] - 4s 340us/step - loss: 0.0560 - acc: 0.8212 - val_loss: 0.4258 - val_acc: 0.7272\n",
      "Epoch 32/100\n",
      "11096/11096 [==============================] - 4s 346us/step - loss: 0.0558 - acc: 0.8215 - val_loss: 0.4311 - val_acc: 0.7294\n",
      "Epoch 33/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.0560 - acc: 0.8181 - val_loss: 0.4512 - val_acc: 0.7301\n",
      "Epoch 34/100\n",
      "11096/11096 [==============================] - 4s 338us/step - loss: 0.0523 - acc: 0.8181 - val_loss: 0.4540 - val_acc: 0.7294\n",
      "Epoch 35/100\n",
      "11096/11096 [==============================] - 4s 333us/step - loss: 0.0499 - acc: 0.8225 - val_loss: 0.4468 - val_acc: 0.7286\n",
      "Epoch 36/100\n",
      "11096/11096 [==============================] - 4s 339us/step - loss: 0.0488 - acc: 0.8224 - val_loss: 0.4646 - val_acc: 0.7290\n",
      "Epoch 37/100\n",
      "11096/11096 [==============================] - 4s 336us/step - loss: 0.0480 - acc: 0.8213 - val_loss: 0.5026 - val_acc: 0.7297\n",
      "Epoch 38/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0454 - acc: 0.8256 - val_loss: 0.4784 - val_acc: 0.7294\n",
      "Epoch 39/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0460 - acc: 0.8216 - val_loss: 0.5030 - val_acc: 0.7362\n",
      "Epoch 40/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0427 - acc: 0.8244 - val_loss: 0.4805 - val_acc: 0.7279\n",
      "Epoch 41/100\n",
      "11096/11096 [==============================] - 4s 333us/step - loss: 0.0431 - acc: 0.8201 - val_loss: 0.5048 - val_acc: 0.7261\n",
      "Epoch 42/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0410 - acc: 0.8262 - val_loss: 0.5244 - val_acc: 0.7344\n",
      "Epoch 43/100\n",
      "11096/11096 [==============================] - 4s 336us/step - loss: 0.0413 - acc: 0.8260 - val_loss: 0.5188 - val_acc: 0.7211\n",
      "Epoch 44/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0400 - acc: 0.8239 - val_loss: 0.5136 - val_acc: 0.7204\n",
      "Epoch 45/100\n",
      "11096/11096 [==============================] - 4s 333us/step - loss: 0.0421 - acc: 0.8240 - val_loss: 0.5158 - val_acc: 0.7189\n",
      "Epoch 46/100\n",
      "11096/11096 [==============================] - 4s 334us/step - loss: 0.0379 - acc: 0.8264 - val_loss: 0.5465 - val_acc: 0.7265\n",
      "Epoch 47/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.0367 - acc: 0.8276 - val_loss: 0.5264 - val_acc: 0.7258\n",
      "Epoch 48/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.0377 - acc: 0.8296 - val_loss: 0.5307 - val_acc: 0.7254\n",
      "Epoch 49/100\n",
      "11096/11096 [==============================] - 4s 330us/step - loss: 0.0369 - acc: 0.8259 - val_loss: 0.5585 - val_acc: 0.7254\n",
      "Epoch 50/100\n",
      "11096/11096 [==============================] - 4s 339us/step - loss: 0.0340 - acc: 0.8286 - val_loss: 0.5303 - val_acc: 0.7312\n",
      "Epoch 51/100\n",
      "11096/11096 [==============================] - 4s 334us/step - loss: 0.0348 - acc: 0.8263 - val_loss: 0.5225 - val_acc: 0.7305\n",
      "Epoch 52/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.0353 - acc: 0.8279 - val_loss: 0.5324 - val_acc: 0.7250\n",
      "Epoch 53/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0339 - acc: 0.8305 - val_loss: 0.5303 - val_acc: 0.7236\n",
      "Epoch 54/100\n",
      "11096/11096 [==============================] - 4s 334us/step - loss: 0.0323 - acc: 0.8280 - val_loss: 0.5580 - val_acc: 0.7348\n",
      "Epoch 55/100\n",
      "11096/11096 [==============================] - 4s 333us/step - loss: 0.0334 - acc: 0.8277 - val_loss: 0.5535 - val_acc: 0.7286\n",
      "Epoch 56/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0330 - acc: 0.8284 - val_loss: 0.5213 - val_acc: 0.7330\n",
      "Epoch 57/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0327 - acc: 0.8280 - val_loss: 0.5123 - val_acc: 0.7250\n",
      "Epoch 58/100\n",
      "11096/11096 [==============================] - 4s 334us/step - loss: 0.0302 - acc: 0.8264 - val_loss: 0.5792 - val_acc: 0.7305\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11096/11096 [==============================] - 4s 338us/step - loss: 0.0314 - acc: 0.8305 - val_loss: 0.5573 - val_acc: 0.7276\n",
      "Epoch 60/100\n",
      "11096/11096 [==============================] - 4s 338us/step - loss: 0.0308 - acc: 0.8305 - val_loss: 0.5854 - val_acc: 0.7333\n",
      "Epoch 61/100\n",
      "11096/11096 [==============================] - 4s 338us/step - loss: 0.0308 - acc: 0.8295 - val_loss: 0.5877 - val_acc: 0.7250\n",
      "Epoch 62/100\n",
      "11096/11096 [==============================] - 4s 334us/step - loss: 0.0294 - acc: 0.8278 - val_loss: 0.5882 - val_acc: 0.7207\n",
      "Epoch 63/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.0325 - acc: 0.8289 - val_loss: 0.5308 - val_acc: 0.7254\n",
      "Epoch 64/100\n",
      "11096/11096 [==============================] - 4s 338us/step - loss: 0.0290 - acc: 0.8290 - val_loss: 0.5719 - val_acc: 0.7225\n",
      "Epoch 65/100\n",
      "11096/11096 [==============================] - 4s 340us/step - loss: 0.0274 - acc: 0.8286 - val_loss: 0.5548 - val_acc: 0.7351\n",
      "Epoch 66/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.0302 - acc: 0.8296 - val_loss: 0.5670 - val_acc: 0.7196\n",
      "Epoch 67/100\n",
      "11096/11096 [==============================] - 4s 338us/step - loss: 0.0298 - acc: 0.8280 - val_loss: 0.5646 - val_acc: 0.7232\n",
      "Epoch 68/100\n",
      "11096/11096 [==============================] - 4s 334us/step - loss: 0.0289 - acc: 0.8301 - val_loss: 0.5758 - val_acc: 0.7323\n",
      "Epoch 69/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0258 - acc: 0.8308 - val_loss: 0.5982 - val_acc: 0.7211\n",
      "Epoch 70/100\n",
      "11096/11096 [==============================] - 4s 338us/step - loss: 0.0291 - acc: 0.8303 - val_loss: 0.5698 - val_acc: 0.7261\n",
      "Epoch 71/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0269 - acc: 0.8312 - val_loss: 0.5989 - val_acc: 0.7258\n",
      "Epoch 72/100\n",
      "11096/11096 [==============================] - 4s 338us/step - loss: 0.0284 - acc: 0.8317 - val_loss: 0.5554 - val_acc: 0.7258\n",
      "Epoch 73/100\n",
      "11096/11096 [==============================] - 4s 334us/step - loss: 0.0270 - acc: 0.8286 - val_loss: 0.5570 - val_acc: 0.7268\n",
      "Epoch 74/100\n",
      "11096/11096 [==============================] - 4s 339us/step - loss: 0.0266 - acc: 0.8316 - val_loss: 0.5528 - val_acc: 0.7276\n",
      "Epoch 75/100\n",
      "11096/11096 [==============================] - 4s 339us/step - loss: 0.0266 - acc: 0.8289 - val_loss: 0.5748 - val_acc: 0.7261\n",
      "Epoch 76/100\n",
      "11096/11096 [==============================] - 4s 340us/step - loss: 0.0263 - acc: 0.8322 - val_loss: 0.5911 - val_acc: 0.7204\n",
      "Epoch 77/100\n",
      "11096/11096 [==============================] - 4s 336us/step - loss: 0.0258 - acc: 0.8305 - val_loss: 0.5991 - val_acc: 0.7240\n",
      "Epoch 78/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0279 - acc: 0.8315 - val_loss: 0.5603 - val_acc: 0.7261\n",
      "Epoch 79/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0267 - acc: 0.8326 - val_loss: 0.5863 - val_acc: 0.7279\n",
      "Epoch 80/100\n",
      "11096/11096 [==============================] - 4s 338us/step - loss: 0.0266 - acc: 0.8322 - val_loss: 0.5848 - val_acc: 0.7186\n",
      "Epoch 81/100\n",
      "11096/11096 [==============================] - 4s 340us/step - loss: 0.0273 - acc: 0.8313 - val_loss: 0.5377 - val_acc: 0.7276\n",
      "Epoch 82/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0243 - acc: 0.8343 - val_loss: 0.5884 - val_acc: 0.7189\n",
      "Epoch 83/100\n",
      "11096/11096 [==============================] - 4s 339us/step - loss: 0.0272 - acc: 0.8280 - val_loss: 0.5686 - val_acc: 0.7229\n",
      "Epoch 84/100\n",
      "11096/11096 [==============================] - 4s 334us/step - loss: 0.0253 - acc: 0.8322 - val_loss: 0.5709 - val_acc: 0.7218\n",
      "Epoch 85/100\n",
      "11096/11096 [==============================] - 4s 338us/step - loss: 0.0264 - acc: 0.8306 - val_loss: 0.5727 - val_acc: 0.7164\n",
      "Epoch 86/100\n",
      "11096/11096 [==============================] - 4s 336us/step - loss: 0.0247 - acc: 0.8312 - val_loss: 0.5537 - val_acc: 0.7211\n",
      "Epoch 87/100\n",
      "11096/11096 [==============================] - 4s 339us/step - loss: 0.0250 - acc: 0.8321 - val_loss: 0.5591 - val_acc: 0.7297\n",
      "Epoch 88/100\n",
      "11096/11096 [==============================] - 4s 336us/step - loss: 0.0247 - acc: 0.8322 - val_loss: 0.5591 - val_acc: 0.7258\n",
      "Epoch 89/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.0239 - acc: 0.8289 - val_loss: 0.5769 - val_acc: 0.7243\n",
      "Epoch 90/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.0229 - acc: 0.8344 - val_loss: 0.5677 - val_acc: 0.7344\n",
      "Epoch 91/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.0257 - acc: 0.8318 - val_loss: 0.5769 - val_acc: 0.7211\n",
      "Epoch 92/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.0245 - acc: 0.8316 - val_loss: 0.5645 - val_acc: 0.7222\n",
      "Epoch 93/100\n",
      "11096/11096 [==============================] - 4s 336us/step - loss: 0.0239 - acc: 0.8326 - val_loss: 0.5831 - val_acc: 0.7218\n",
      "Epoch 94/100\n",
      "11096/11096 [==============================] - 4s 343us/step - loss: 0.0237 - acc: 0.8312 - val_loss: 0.5885 - val_acc: 0.7175\n",
      "Epoch 95/100\n",
      "11096/11096 [==============================] - 4s 336us/step - loss: 0.0254 - acc: 0.8329 - val_loss: 0.5742 - val_acc: 0.7301\n",
      "Epoch 96/100\n",
      "11096/11096 [==============================] - 4s 337us/step - loss: 0.0229 - acc: 0.8346 - val_loss: 0.5934 - val_acc: 0.7243\n",
      "Epoch 97/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0238 - acc: 0.8316 - val_loss: 0.5483 - val_acc: 0.7294\n",
      "Epoch 98/100\n",
      "11096/11096 [==============================] - 4s 333us/step - loss: 0.0226 - acc: 0.8317 - val_loss: 0.5535 - val_acc: 0.7268\n",
      "Epoch 99/100\n",
      "11096/11096 [==============================] - 4s 335us/step - loss: 0.0217 - acc: 0.8338 - val_loss: 0.5869 - val_acc: 0.7232\n",
      "Epoch 100/100\n",
      "11096/11096 [==============================] - 4s 334us/step - loss: 0.0247 - acc: 0.8298 - val_loss: 0.5482 - val_acc: 0.7247\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "            pad_vec_data(corpus), \n",
    "            labels, \n",
    "            test_size=0.2, \n",
    "            random_state=3945)\n",
    "\n",
    "train_x = np.array(train_x)\n",
    "test_x = np.array(test_x)\n",
    "train_y = np.array(train_y)\n",
    "test_y = np.array(test_y)\n",
    "gc.collect()\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          epochs=nb_epochs,\n",
    "          validation_data=(test_x, test_y),\n",
    "#                   verbose=0,\n",
    "          callbacks=[\n",
    "#                       EarlyStopping(min_delta=0.000025, patience=10),\n",
    "          ])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE: hello there, my name is mike [9.991887e-01 8.112461e-04]\n",
      "NEGATIVE: not something i wanted to happen but i think i need to add more words for it to see [1.3842149e-05 9.9998617e-01]\n",
      "NEGATIVE: this was something i wanted i'm grateful for this app and i am looking forward to more things from you guys [0.02264562 0.97735435]\n",
      "NEGATIVE: i don't fucking like you man [0.00726484 0.99273515]\n",
      "POSITIVE: considering how the usual process for this is tedious, having an app like this solves a lot of problems [0.98350775 0.01649221]\n",
      "POSITIVE: thank you that was some wonderful service that really helped me get from point A to point B [0.90331525 0.09668473]\n",
      "POSITIVE: this is amazing and was able to shorten the amount of time i needed to take to achieve this [0.9834714  0.01652856]\n",
      "NEGATIVE: this is nothing short of amazing, I cannot believe it [0.01947779 0.9805222 ]\n",
      "NEGATIVE: how are you [0.2729738 0.7270262]\n",
      "NEGATIVE: i really like you [4.0769982e-04 9.9959236e-01]\n",
      "NEGATIVE: i don't actually like this product! [0.1846507 0.8153493]\n",
      "POSITIVE: it's useless if you can't use it! [0.9968104  0.00318959]\n",
      "NEGATIVE: i can't believe i never heard of this [0.07903263 0.92096734]\n",
      "NEGATIVE: this is awesome, didn't know I needed this [0.00199255 0.9980075 ]\n",
      "POSITIVE: so what am i supposed to use this for? [0.95026135 0.0497386 ]\n",
      "NEGATIVE: what do I need this for? [0.02713419 0.97286576]\n",
      "POSITIVE: this is good [0.97997355 0.02002641]\n",
      "NEGATIVE: this is not good [0.01879587 0.9812041 ]\n",
      "NEGATIVE: although the movie was great, it lacked impact [4.1911659e-05 9.9995804e-01]\n",
      "NEGATIVE: the movie wasnt that nice [0.02440769 0.9755923 ]\n",
      "POSITIVE: the movie was nice [0.97038394 0.02961606]\n",
      "NEGATIVE: this is not acceptable, I lost everything using your app [0.0167283 0.9832718]\n",
      "NEGATIVE: that was kinda stupid [0.01144154 0.98855853]\n",
      "POSITIVE: the instructions were unlear and is not friendly for non-techy people [0.6367832  0.36321682]\n",
      "NEGATIVE: this is really useful i would definitely tell everyone about it [2.0822986e-04 9.9979180e-01]\n",
      "NEGATIVE: i need to try this! [0.20163661 0.7983633 ]\n",
      "NEGATIVE: which is your favourite harry potter filmsorcerers stonechamber of secretsprisoner of azkabangoblet of firei like them all equallyi hate harry potter and think this is a stupid question [7.747129e-05 9.999225e-01]\n",
      "NEGATIVE: sitting in the third row of the imax cinema at sydney s darling harbour  but i sometimes felt as though i was in the tiny two seater plane that carried the giant camera around australia  sweeping and gliding  banking and hovering over some of the most not [1.4840807e-04 9.9985158e-01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mess = [\n",
    "    \"hello there, my name is mike\",\n",
    "    \"not something i wanted to happen but i think i need to add more words for it to see\",\n",
    "    \"this was something i wanted i'm grateful for this app and i am looking forward to more things from you guys\",\n",
    "    \"i don't fucking like you man\",\n",
    "    \"considering how the usual process for this is tedious, having an app like this solves a lot of problems\",\n",
    "    \"thank you that was some wonderful service that really helped me get from point A to point B\",\n",
    "    \"this is amazing and was able to shorten the amount of time i needed to take to achieve this\",\n",
    "    \"this is nothing short of amazing, I cannot believe it\",\n",
    "    \"how are you\",\n",
    "    \"i really like you\",\n",
    "    \"i don't actually like this product!\",\n",
    "    \"it's useless if you can't use it!\",\n",
    "    \"i can't believe i never heard of this\",\n",
    "    \"this is awesome, didn't know I needed this\",\n",
    "    \"so what am i supposed to use this for?\",\n",
    "    \"what do I need this for?\",\n",
    "    \"this is good\",\n",
    "    \"this is not good\",\n",
    "    \"although the movie was great, it lacked impact\",\n",
    "    \"the movie wasnt that nice\",\n",
    "    \"the movie was nice\",\n",
    "    \"this is not acceptable, I lost everything using your app\",\n",
    "    \"that was kinda stupid\",\n",
    "    \"the instructions were unlear and is not friendly for non-techy people\",\n",
    "    \"this is really useful i would definitely tell everyone about it\",\n",
    "    \"i need to try this!\",\n",
    "    \"which is your favourite harry potter filmsorcerers stonechamber of secretsprisoner of azkabangoblet of firei like them all equallyi hate harry potter and think this is a stupid question\",\n",
    "    \"sitting in the third row of the imax cinema at sydney s darling harbour  but i sometimes felt as though i was in the tiny two seater plane that carried the giant camera around australia  sweeping and gliding  banking and hovering over some of the most not\",\n",
    "]\n",
    "\n",
    "pred = model.predict(pad_vec_data(list(map(remove_stop_words, mess))))\n",
    "output = ''\n",
    "\n",
    "for i ,m in enumerate(mess):\n",
    "    output += ('{} {} {}\\n'.format('POSITIVE:' if pred[i][0] > 0.5 else 'NEGATIVE:', m, pred[i]))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
