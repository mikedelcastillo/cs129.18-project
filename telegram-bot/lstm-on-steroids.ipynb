{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk.corpus import stopwords\n",
    "np.random.seed(1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['only' 'y' 'by' 'am' 'most' 'me' 'same' 'these' 'so' 'some' 'why' 'down'\n",
      " 'had' 'd' 'at' 'having' 'those' 'has' 'few' 'theirs' \"you've\" 'more' 'i'\n",
      " 'than' 'through' 'be' 'what' 'where' 'myself' 'which' 'doing' 'ours'\n",
      " 'will' 'in' 'both' 'do' 'it' 'o' 'on' 'yours' 'once' 'ourselves' 'here'\n",
      " 'about' \"it's\" 'my' 'for' 'her' 'then' 'after' \"should've\" 'from' 'each'\n",
      " 'when' 'does' 'now' 'off' 'don' 'are' 'we' 'itself' 'should' 'his'\n",
      " 'between' 'our' 'were' 'under' 'other' 'all' 'she' 'won' 'been' \"you're\"\n",
      " 'how' 'did' 'yourself' 'they' 'into' 'there' 've' 'such' 't' 's' 'and'\n",
      " 'over' 'to' 'just' 'was' 'being' 'because' 'if' 'who' 'further' 'the'\n",
      " 'any' \"that'll\" 'themselves' 'as' 'again' \"you'd\" 'until' 'he' 'him'\n",
      " 'this' 'or' 'of' 'below' 'an' \"she's\" 'weren' 'm' 'their' 'ma' 'up' 'll'\n",
      " 'whom' 'hers' 'can' 'you' 'them' 'very' 'a' 'herself' 'before' 'too'\n",
      " 'himself' 'during' 're' 'out' 'its' 'above' 'own' 'have' 'while'\n",
      " 'yourselves' 'that' 'with' \"you'll\" 'is' 'your']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "stop_words = pd.read_csv('../data/stopwords.csv')['words'].values\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    word_tokens = text_to_word_sequence(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222914"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([pd.read_csv(x) for x in [\n",
    "    '../data/tweets.csv',\n",
    "    '../data/word-list.csv',\n",
    "    '../data/twitter-airline-sentiment.csv',\n",
    "    '../data/sentiwordnet.csv',\n",
    "    '../data/reviews.csv',\n",
    "    '../data/imdb.csv',\n",
    "\n",
    "]])\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nbc commissary fountain little things like mak...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vanilla really good fish tacos</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drinkin listening typical night</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>incredible taste hard believe zero contains no...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doesnt make want vanilla coke zero dont know a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  pos  neg\n",
       "0  nbc commissary fountain little things like mak...  0.0  1.0\n",
       "1                     vanilla really good fish tacos  0.0  1.0\n",
       "2                    drinkin listening typical night  0.0  1.0\n",
       "3  incredible taste hard believe zero contains no...  0.0  1.0\n",
       "4  doesnt make want vanilla coke zero dont know a...  0.0  1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = list(map(remove_stop_words, data['text'].values))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222914"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = data['text'].values\n",
    "corpus = [text_to_word_sequence(y) for y in sentences]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw_train, X_raw_test, Y_train, Y_test = train_test_split(\n",
    "    sentences,\n",
    "    data[['pos', 'neg']].values,\n",
    "    test_size=0.2, \n",
    "    random_state=3945\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 35\n",
    "vector_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import os\n",
    "\n",
    "# word2vec = KeyedVectors.load_word2vec_format('../../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "word2vec_name = 'lstm-word2vec.bin'\n",
    "\n",
    "# Create Word2Vec\n",
    "if os.path.isfile(word2vec_name): \n",
    "    print(\"Loading...\")\n",
    "    word2vec = KeyedVectors.load(word2vec_name)\n",
    "\n",
    "else:\n",
    "    print(\"Computing...\")\n",
    "    word2vec = Word2Vec(sentences=corpus,\n",
    "                    size=vector_size, \n",
    "                    window=10, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=4)\n",
    "    word2vec.save(word2vec_name)\n",
    "\n",
    "word2vec = word2vec.wv\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2index(corpus):\n",
    "    gc.collect()\n",
    "    input_matrix = np.zeros((len(corpus),max_sentence_length))\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        for t, token in enumerate(corpus[i]):\n",
    "            if t >= max_sentence_length:\n",
    "                break\n",
    "            if token not in word2vec.vocab:\n",
    "                continue\n",
    "            input_matrix[i, t] = word2vec.vocab.get(token).index\n",
    "    return input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sent2index(X_raw_train)\n",
    "X_test = sent2index(X_raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60229, 100)\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "vocab_len = len(word2vec.vocab) + 1\n",
    "\n",
    "emb_matrix = np.zeros((vocab_len, vector_size))\n",
    "\n",
    "for word in word2vec.vocab:\n",
    "    index = word2vec.vocab.get(word).index\n",
    "    emb_matrix[index, :] = word2vec[word]\n",
    "    \n",
    "print(emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "sentence_indices = Input(shape=(max_sentence_length,))\n",
    "    \n",
    "embedding_layer = Embedding(vocab_len, vector_size, trainable = False)\n",
    "embedding_layer.build((None,))\n",
    "embedding_layer.set_weights([emb_matrix])\n",
    "\n",
    "embeddings = embedding_layer(sentence_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35, 100)           6022900   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 35, 128)           117248    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 6,271,990\n",
      "Trainable params: 249,090\n",
      "Non-trainable params: 6,022,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X = LSTM(128, return_sequences=True)(embeddings)\n",
    "X = Dropout(0.5)(X)\n",
    "X = LSTM(128, return_sequences=False)(X)\n",
    "X = Dropout(0.5)(X)\n",
    "X = Dense(2, activation=None)(X)\n",
    "X = Activation('softmax')(X)\n",
    "\n",
    "model = Model(inputs=[sentence_indices], outputs=X)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"keras-model.h5\"\n",
    "\n",
    "def compile_model():\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "def train_model(): \n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y=Y_train, \n",
    "        batch_size=512, \n",
    "        epochs=100, \n",
    "        verbose=1, \n",
    "        validation_data=(X_test, Y_test))\n",
    "    \n",
    "    print(\"Saving model\")\n",
    "    model.save_weights(model_file)\n",
    "\n",
    "def load_model():\n",
    "    model.load_weights(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Train on 178331 samples, validate on 44583 samples\n",
      "Epoch 1/100\n",
      "178331/178331 [==============================] - 45s 250us/step - loss: 0.4277 - acc: 0.4685 - val_loss: 0.4268 - val_acc: 0.2969\n",
      "Epoch 2/100\n",
      "178331/178331 [==============================] - 43s 241us/step - loss: 0.4238 - acc: 0.4996 - val_loss: 0.4224 - val_acc: 0.4373\n",
      "Epoch 3/100\n",
      "178331/178331 [==============================] - 43s 241us/step - loss: 0.4163 - acc: 0.5068 - val_loss: 0.4086 - val_acc: 0.5426\n",
      "Epoch 4/100\n",
      "178331/178331 [==============================] - 43s 241us/step - loss: 0.4062 - acc: 0.5017 - val_loss: 0.4016 - val_acc: 0.4526\n",
      "Epoch 5/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.4006 - acc: 0.5046 - val_loss: 0.3970 - val_acc: 0.5087\n",
      "Epoch 6/100\n",
      "178331/178331 [==============================] - 43s 241us/step - loss: 0.3967 - acc: 0.5117 - val_loss: 0.3948 - val_acc: 0.5131\n",
      "Epoch 7/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3942 - acc: 0.5235 - val_loss: 0.3949 - val_acc: 0.4655\n",
      "Epoch 8/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3910 - acc: 0.5256 - val_loss: 0.3905 - val_acc: 0.5334\n",
      "Epoch 9/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3890 - acc: 0.5389 - val_loss: 0.3938 - val_acc: 0.4471\n",
      "Epoch 10/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3872 - acc: 0.5399 - val_loss: 0.3879 - val_acc: 0.5787\n",
      "Epoch 11/100\n",
      "178331/178331 [==============================] - 43s 241us/step - loss: 0.3847 - acc: 0.5513 - val_loss: 0.3885 - val_acc: 0.5575\n",
      "Epoch 12/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3835 - acc: 0.5479 - val_loss: 0.3861 - val_acc: 0.5990\n",
      "Epoch 13/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3813 - acc: 0.5480 - val_loss: 0.3840 - val_acc: 0.5947\n",
      "Epoch 14/100\n",
      "178331/178331 [==============================] - 43s 241us/step - loss: 0.3795 - acc: 0.5531 - val_loss: 0.3848 - val_acc: 0.5067\n",
      "Epoch 15/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3780 - acc: 0.5540 - val_loss: 0.3839 - val_acc: 0.5957\n",
      "Epoch 16/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3775 - acc: 0.5539 - val_loss: 0.3853 - val_acc: 0.5469\n",
      "Epoch 17/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3751 - acc: 0.5602 - val_loss: 0.3834 - val_acc: 0.5405\n",
      "Epoch 18/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3735 - acc: 0.5581 - val_loss: 0.3844 - val_acc: 0.5685\n",
      "Epoch 19/100\n",
      "178331/178331 [==============================] - 43s 241us/step - loss: 0.3721 - acc: 0.5638 - val_loss: 0.3807 - val_acc: 0.5496\n",
      "Epoch 20/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3710 - acc: 0.5681 - val_loss: 0.3810 - val_acc: 0.5777\n",
      "Epoch 21/100\n",
      "178331/178331 [==============================] - 43s 241us/step - loss: 0.3689 - acc: 0.5708 - val_loss: 0.3805 - val_acc: 0.5961\n",
      "Epoch 22/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3674 - acc: 0.5697 - val_loss: 0.3792 - val_acc: 0.5576\n",
      "Epoch 23/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3662 - acc: 0.5722 - val_loss: 0.3796 - val_acc: 0.5761\n",
      "Epoch 24/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3649 - acc: 0.5721 - val_loss: 0.3785 - val_acc: 0.5735\n",
      "Epoch 25/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3637 - acc: 0.5762 - val_loss: 0.3792 - val_acc: 0.5527\n",
      "Epoch 26/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3621 - acc: 0.5744 - val_loss: 0.3798 - val_acc: 0.5307\n",
      "Epoch 27/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3607 - acc: 0.5729 - val_loss: 0.3825 - val_acc: 0.6029\n",
      "Epoch 28/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3591 - acc: 0.5778 - val_loss: 0.3797 - val_acc: 0.5841\n",
      "Epoch 29/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3578 - acc: 0.5784 - val_loss: 0.3786 - val_acc: 0.6011\n",
      "Epoch 30/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3562 - acc: 0.5847 - val_loss: 0.3803 - val_acc: 0.5575\n",
      "Epoch 31/100\n",
      "178331/178331 [==============================] - 44s 244us/step - loss: 0.3563 - acc: 0.5816 - val_loss: 0.3781 - val_acc: 0.5940\n",
      "Epoch 32/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3536 - acc: 0.5845 - val_loss: 0.3793 - val_acc: 0.5568\n",
      "Epoch 33/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3531 - acc: 0.5857 - val_loss: 0.3810 - val_acc: 0.5677\n",
      "Epoch 34/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3509 - acc: 0.5885 - val_loss: 0.3827 - val_acc: 0.5922\n",
      "Epoch 35/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3506 - acc: 0.5910 - val_loss: 0.3809 - val_acc: 0.6355\n",
      "Epoch 36/100\n",
      "178331/178331 [==============================] - 43s 244us/step - loss: 0.3493 - acc: 0.5930 - val_loss: 0.3795 - val_acc: 0.5758\n",
      "Epoch 37/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3477 - acc: 0.5932 - val_loss: 0.3799 - val_acc: 0.5186\n",
      "Epoch 38/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3466 - acc: 0.5919 - val_loss: 0.3810 - val_acc: 0.5803\n",
      "Epoch 39/100\n",
      "178331/178331 [==============================] - 43s 244us/step - loss: 0.3459 - acc: 0.5958 - val_loss: 0.3848 - val_acc: 0.5810\n",
      "Epoch 40/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3445 - acc: 0.5964 - val_loss: 0.3812 - val_acc: 0.5542\n",
      "Epoch 41/100\n",
      "178331/178331 [==============================] - 43s 244us/step - loss: 0.3441 - acc: 0.5910 - val_loss: 0.3832 - val_acc: 0.5852\n",
      "Epoch 42/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3424 - acc: 0.5957 - val_loss: 0.3877 - val_acc: 0.5842\n",
      "Epoch 43/100\n",
      "178331/178331 [==============================] - 43s 244us/step - loss: 0.3411 - acc: 0.5977 - val_loss: 0.3837 - val_acc: 0.5795\n",
      "Epoch 44/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3404 - acc: 0.5955 - val_loss: 0.3806 - val_acc: 0.5544\n",
      "Epoch 45/100\n",
      "178331/178331 [==============================] - 44s 244us/step - loss: 0.3390 - acc: 0.5983 - val_loss: 0.3871 - val_acc: 0.5898\n",
      "Epoch 46/100\n",
      "178331/178331 [==============================] - 44s 244us/step - loss: 0.3374 - acc: 0.6025 - val_loss: 0.3884 - val_acc: 0.5609\n",
      "Epoch 47/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3372 - acc: 0.6016 - val_loss: 0.3881 - val_acc: 0.5530\n",
      "Epoch 48/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3367 - acc: 0.6048 - val_loss: 0.3910 - val_acc: 0.5415\n",
      "Epoch 49/100\n",
      "178331/178331 [==============================] - 43s 244us/step - loss: 0.3351 - acc: 0.6060 - val_loss: 0.3886 - val_acc: 0.5757\n",
      "Epoch 50/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3335 - acc: 0.6051 - val_loss: 0.3863 - val_acc: 0.5924\n",
      "Epoch 51/100\n",
      "178331/178331 [==============================] - 44s 245us/step - loss: 0.3327 - acc: 0.6032 - val_loss: 0.3882 - val_acc: 0.5710\n",
      "Epoch 52/100\n",
      "178331/178331 [==============================] - 44s 245us/step - loss: 0.3314 - acc: 0.6043 - val_loss: 0.3951 - val_acc: 0.6248\n",
      "Epoch 53/100\n",
      "178331/178331 [==============================] - 43s 244us/step - loss: 0.3311 - acc: 0.6101 - val_loss: 0.3902 - val_acc: 0.5322\n",
      "Epoch 54/100\n",
      "178331/178331 [==============================] - 44s 245us/step - loss: 0.3302 - acc: 0.6077 - val_loss: 0.3895 - val_acc: 0.5098\n",
      "Epoch 55/100\n",
      "178331/178331 [==============================] - 43s 244us/step - loss: 0.3291 - acc: 0.6087 - val_loss: 0.3948 - val_acc: 0.5494\n",
      "Epoch 56/100\n",
      "178331/178331 [==============================] - 44s 244us/step - loss: 0.3290 - acc: 0.6110 - val_loss: 0.3903 - val_acc: 0.5671\n",
      "Epoch 57/100\n",
      "178331/178331 [==============================] - 44s 244us/step - loss: 0.3270 - acc: 0.6133 - val_loss: 0.3959 - val_acc: 0.5958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3271 - acc: 0.6120 - val_loss: 0.3934 - val_acc: 0.5619\n",
      "Epoch 59/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3256 - acc: 0.6143 - val_loss: 0.3961 - val_acc: 0.5641\n",
      "Epoch 60/100\n",
      "178331/178331 [==============================] - 44s 246us/step - loss: 0.3241 - acc: 0.6146 - val_loss: 0.4007 - val_acc: 0.5744\n",
      "Epoch 61/100\n",
      "178331/178331 [==============================] - 44s 246us/step - loss: 0.3248 - acc: 0.6124 - val_loss: 0.3965 - val_acc: 0.5965\n",
      "Epoch 62/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3225 - acc: 0.6142 - val_loss: 0.3998 - val_acc: 0.6129\n",
      "Epoch 63/100\n",
      "178331/178331 [==============================] - 43s 243us/step - loss: 0.3228 - acc: 0.6149 - val_loss: 0.3972 - val_acc: 0.5567\n",
      "Epoch 64/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3214 - acc: 0.6138 - val_loss: 0.3949 - val_acc: 0.5874\n",
      "Epoch 65/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3209 - acc: 0.6167 - val_loss: 0.3931 - val_acc: 0.5774\n",
      "Epoch 66/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3195 - acc: 0.6182 - val_loss: 0.3971 - val_acc: 0.5759\n",
      "Epoch 67/100\n",
      "178331/178331 [==============================] - 43s 242us/step - loss: 0.3193 - acc: 0.6189 - val_loss: 0.3979 - val_acc: 0.5614\n",
      "Epoch 68/100\n",
      " 46080/178331 [======>.......................] - ETA: 29s - loss: 0.3169 - acc: 0.6160"
     ]
    }
   ],
   "source": [
    "force_train = False\n",
    "\n",
    "if os.path.isfile(model_file) and not force_train:\n",
    "    print(\"Loading model...\")\n",
    "    load_model()\n",
    "    compile_model()\n",
    "else:\n",
    "    print(\"Training model...\")\n",
    "    compile_model()\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess = np.array([\n",
    "    'this is slow',\n",
    "    'this is exceptional service',\n",
    "])\n",
    "X_test_indices = sent2index(mess)\n",
    "pred = model.predict(X_test_indices)\n",
    "\n",
    "output = ''\n",
    "for i ,m in enumerate(mess):\n",
    "        output += ('{} {} {}\\n'.format('POSITIVE:' if pred[i][0] > 0.5 else 'NEGATIVE:', m, pred[i]))\n",
    "\n",
    "        \n",
    "print(output)\n",
    "        \n",
    "del mess\n",
    "del output\n",
    "del pred\n",
    "del X_test_indices\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "import urllib, json\n",
    "import urllib.request\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "TLGRM_SECRET = os.getenv('TLGRM_SECRET')\n",
    "TLGRM_MIKE = os.getenv('TLGRM_MIKE')\n",
    "\n",
    "def telegram_call(method, query = {}):\n",
    "    try:\n",
    "        url = 'http://api.telegram.org/bot{}/{}?{}'.format(TLGRM_SECRET, method, urllib.parse.urlencode(query))\n",
    "        response = urllib.request.urlopen(url)\n",
    "        return json.loads(response.read().decode(\"utf-8\"))\n",
    "    except:\n",
    "        print('Repeating call...')\n",
    "        return telegram_call(method, query)\n",
    "\n",
    "def telegram_bot(respond):\n",
    "    last_offset = 0\n",
    "    \n",
    "    while True:\n",
    "        data = telegram_call('getupdates', {'offset': last_offset})\n",
    "        for item in data['result']:\n",
    "            last_offset = item['update_id'] + 1\n",
    "            if 'message' in item:\n",
    "                if 'text' in item['message']:\n",
    "                    text = item['message']['text']\n",
    "                    chat_id = item['message']['chat']['id']\n",
    "                    response = respond(text)\n",
    "                    smd = telegram_call('sendmessage', {\n",
    "                        'chat_id': chat_id,\n",
    "                        'text': response\n",
    "                    })\n",
    "                    \n",
    "                    while smd['ok'] == False:\n",
    "                        smd = telegram_call('sendmessage', {\n",
    "                            'chat_id': chat_id,\n",
    "                            'text': response\n",
    "                        })\n",
    "                        \n",
    "                    clear_output()\n",
    "                    print('FROM: {}\\nSAYS: {}\\nRESPONSE: {}\\n\\n'.format(chat_id, text, response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "gc.collect()\n",
    "\n",
    "uhms = [\n",
    "    \"uhm\", \"uhh\", \"hmm\", \n",
    "    \"hmmm\", \"oh uh\", \"oh hmm\",\n",
    "]\n",
    "\n",
    "sentiment = [\n",
    "    \"😭\",\n",
    "    \"☹️\",\n",
    "    \"😐\",\n",
    "    \"🙂\",\n",
    "    \"😍\",\n",
    "]\n",
    "\n",
    "def respond(text):\n",
    "    mess = np.array([text])\n",
    "    X_test_indices = sent2index(mess)\n",
    "    pred = model.predict(X_test_indices)\n",
    "    score = pred[0][0]\n",
    "    sent = sentiment[floor(score * 5)]\n",
    "    response = '{}: {}'.format(sent, text)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telegram_bot(respond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
